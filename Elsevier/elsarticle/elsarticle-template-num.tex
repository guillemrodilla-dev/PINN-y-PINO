%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 249 2024-04-06 10:51:24Z rishi $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath, amsthm, amsfonts}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{defi}{Definition}


%% Commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\Ltwo}{L^2}
\newcommand{\Ltwod}{L^2(D)}
\newcommand{\Lp}[1]{L^{#1}}
\newcommand{\Hone}{H^1}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left| #1 \right|}

\newcommand{\E}{\mathbb{E}}

\newcommand{\G}{\mathcal{G}}
\renewcommand{\Gt}{\mathcal{G}_{\theta}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\F}{\mathcal{F}}

\newcommand{\Nop}{\mathcal{N}}
\newcommand{\Bop}{\mathcal{B}}

\newcommand{\Ldata}{\mathcal{L}_{\mathrm{data}}}
\newcommand{\Lphys}{\mathcal{L}_{\mathrm{phys}}}
\newcommand{\Lbc}{\mathcal{L}_{\mathrm{bc}}}
\newcommand{\Ltot}{\mathcal{L}_{\mathrm{total}}}

\newcommand{\Jdata}{\mathcal{J}_{\mathrm{data}}}
\newcommand{\Jphys}{\mathcal{J}_{\mathrm{phys}}}
\newcommand{\Jbc}{\mathcal{J}_{\mathrm{bc}}}
\newcommand{\Jtot}{\mathcal{J}_{\mathrm{total}}}

\newcommand{\D}{\mathcal{D}}
\newcommand{\supp}{\operatorname{supp}}


\journal{Journal of Computational Physics}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{PINNs and PINOs}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author{Tafalla-Domínguez, Wenceslao} %% Author name

%% Author affiliation
\affiliation{organization={Estudiante del Máster INVEST-MAT, Universitat Politècnica de València, Universitat de València},%Department and Organization
            addressline={}, 
            city={València},
            postcode={}, 
            state={},
            country={Spain}}

\author{Rodilla-Alamà, Guillem} %% Author name

%% Author affiliation
\affiliation{organization={Estudiante del Máster INVEST-MAT, Universitat Politècnica de València, Universitat de València},%Department and Organization
            addressline={}, 
            city={València},
            postcode={}, 
            state={},
            country={Spain}}

\author{Pereira-García, Adrián} %% Author name

%% Author affiliation
\affiliation{organization={Estudiante del Máster INVEST-MAT, Universitat Politècnica de València, Universitat de València},%Department and Organization
            addressline={}, 
            city={València},
            postcode={}, 
            state={},
            country={Spain}}

%% Abstract
\begin{abstract}
PIN y PON
\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics{grabs}
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item Son unos fieras
%\item Menudos cracks
%\end{highlights}

%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
PINN \sep PINO \sep Deep Learning \sep Neural Networks \sep Scientific Machine Learning

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%
\section{Limitations of Physics-Informed Neural Networks}

Physics-Informed Neural Networks (PINNs) provide a flexible framework for incorporating physical laws into neural network training. However, despite their conceptual appeal, several practical limitations have been widely reported in the literature.

A well-known issue is their low-frequency (spectral) bias, which makes PINNs struggle to accurately represent high-frequency components of the solution. As a consequence, they perform poorly on PDEs modeling multi-scale phenomena or involving sharp spatial and temporal variations.

There are also issues related to how information from initial and boundary conditions propagates through the domain. As the distance from the boundaries or the initial time increases, the influence of these constraints may fade, which can noticeably affect accuracy in large domains or long-time simulations.

Scalability is another important concern. As the dimensionality of the problem or the size of the domain grows, training becomes more expensive and, in many cases, harder to stabilize. 

As a result, PINNs' performance depends strongly on the availability of additional data, careful tuning of loss weights, and problem-specific heuristics.

Taken together, these limitations motivate the exploration of alternative approaches that move beyond solution-based learning toward operator-level representations, which we formalize in the following section.


\section{Problem Formulation}

We consider two broad classes of partial differential equations: stationary systems and dynamical systems. In both cases, the objective is to approximate the corresponding solution operator mapping input functions to PDE solutions.

\subsection{Stationary Problems}

Let $D \subset \R^d$ be a bounded domain with boundary $\partial D$. We consider stationary PDEs of the form

\begin{equation}\label{eq:stationary_pde}
\begin{aligned}
\P(u(x),a(x)) &= 0,  &x \in D\subset\R^d,\\
u(x) &= g(x), &x \in \partial D,
\end{aligned}
\end{equation}

where $a \in \A$ denotes an input function parameterizing the equation, $u \in \U$ is the unknown of the equation and $g$ is a fixed boundary condition. $\P:\A\times\U\to\F$ is a partial differential operator (possibly non-linear). Here $\U$ and $\F$ are Banach spaces of functions and $\A$ is a subset of a Banach space.
The solution operator associated with the stationary problem is defined as
\begin{equation}\label{eq:stationary_operator}
\G^\dagger : \A \to \U, 
\qquad
\G^\dagger(a) = u.
\end{equation}

\subsection{Dynamical Problems}

We next consider time-dependent PDEs defined on the space-time domain $D \times (0,\infty)$. The dynamical system is given by
\begin{equation}\label{eq:dynamic_pde}
\begin{aligned}
\frac{\partial u}{\partial t}(x,t) &= \mathcal{R}(u(x,t)), 
 &(x,t) \in D \times (0,\infty),\\
u(x,t) &= g(x,t),  &(x,t) \in \partial D \times (0,\infty),\\
u(x,0) &= a(x),  &x \in \overline{D}.
\end{aligned}
\end{equation}

Here, the input function $a \in \mathcal{A}$ corresponds to the initial condition of the system, $u(\cdot,t)\in\U$ for $t>0$ is the unknown and $g$ is a fixed boundary condition. $\mathcal{R}$ denotes a (possibly nonlinear) differential operator governing the dynamics. Typical examples include the Burgers equation and the incompressible Navier--Stokes equations.

The solution operator associated with the dynamic problem is defined as
\begin{equation}\label{eq:dynamic_operator}
\G^\dagger : \A \to \mathcal{C}((0,T];\U), 
\qquad
\G^\dagger(a) = u.
\end{equation}

In both the stationary and dynamical settings, the objective is to approximate the solution operator $\G^\dagger$ rather than individual solution realizations. This operator-learning perspective forms the foundation of neural operators and, in particular, Physics-Informed Neural Operators introduced in the following sections.


\subsection{Neural Operators}

Neural operators aim to approximate the solution operators (\ref{eq:stationary_operator}) and (\ref{eq:dynamic_operator}). We now present the structure of a neural operator.
\begin{defi}[Neural operator]
    A neural operator is defined as
    \begin{equation}\label{eq:neural_operator}
        \bf \Gt := Q \circ \sigma(W_L + K_L)\circ\dots\circ\sigma(W_1 +K_1)\circ P,
    \end{equation}
    where $\bf P$ is the lift operator, it sends the lower dimension input function into a higher dimensional space, and $\bf Q$ is the projection operator, it sends the higher dimension function back to the lower dimensional space of the solution function. Both operators are parameterized with neural networks. Between those operators, the model has L layers of the form $\sigma(W_l+K_l)$. Here $W_l$ are linear operators, parameterized with matrices, $\sigma$ is the activation function (fixed), and $K_l$ are called kernel operators. The parameters $\theta$ are $P,Q,W_l,K_l$.
\end{defi}


\begin{defi}[Kernel Integral Operator]
    Let $\kappa^{(l)}\in C(D\times D,\R^{d_{l+1}\times d_l})$ and let $\nu$ be a Borel measure on D. The we define the kernel integral operator $K$ mentioned above as
    \begin{equation}
        (Kv_l)(x) = \int_D \kappa^{(l)}(x,y)v_l(y)\ d\nu(y)\quad \forall x\in D.
    \end{equation}
\end{defi}

We are going to work with one particular form of the kernel integral operator called Fourier convolution operator defined by
\begin{equation}
    (Kv_l)(x) = F^{-1}\big(R\cdot (F v_l)\big ) \quad \forall x\in D.
\end{equation}

where $F,F^{-1}$ are the FFT and its inverse respectively, and R is part of the parameter $\theta$.

\subsection{Training neural operators}


Our goal is to train a neural operator $\Gt$ that accurately approximates the solution operator $\G^\dagger$.
This is achieved by solving an optimization problem in which the parameters $\theta$ are obtained by minimizing a loss functional that combines data fidelity with physical consistency.

\paragraph{Data loss.}
We assume access to a dataset $\{a_j, u_j\}_{j=1}^N$, where $u_j = \G^\dagger(a_j)$ and the inputs $a_j$ are independent and identically distributed samples drawn from a probability measure $\mu$ supported on $\A$.
For the stationary problem, the pointwise data loss is defined as
\begin{equation}
    \Ldata(u, \Gt(a))
    := \norm{u - \Gt(a)}_{\U}^2
    = \int_D \abs{u(x) - \Gt(a)(x)}^2 \, \mathrm{d}x .
\end{equation}
The corresponding operator-level data loss is obtained by averaging over the input distribution,
\begin{equation}
    \Jdata(\Gt)
    := \E_{a \sim \mu} \big[ \Ldata(u, \Gt(a)) \big]
    \approx \frac{1}{N} \sum_{j=1}^N \int_D 
    \abs{u_j(x) - \Gt(a_j)(x)}^2 \, \mathrm{d}x .
\end{equation}
The extension to time-dependent problems is completely analogous, with the loss evaluated over the space--time domain.

\paragraph{Physics-informed loss: stationary problems.}
To enforce that the learned operator is consistent with the underlying physics, PINOs incorporate the governing PDE directly into the loss function.
For the stationary problem \eqref{eq:stationary_pde}, the physical residual is obtained by evaluating the differential operator $\P$ at the network prediction $\Gt(a)$.
The physics-informed loss is then defined as
\begin{equation}
    \Lphys(a,\Gt(a)) := \int_D |\P(\Gt(a)(x),a(x))|^2dx+
            \alpha \int_{\partial D}|\Gt(a)(x)-g(x)|^2dx ,
\end{equation}
where the hyper-parameter $\alpha>0$ balances the relative importance of satisfying the boundary conditions versus the interior PDE residual during optimization. 

By averaging over the input distribution, we obtain the operator physics loss
\begin{equation}
    \Jphys(\Gt)
    := \E_{a \sim \mu} \big[ \Lphys(a, \Gt(a)) \big] .
\end{equation}

\paragraph{Physics-informed loss: dynamical problems.}
The same construction applies to time-dependent PDEs of the form \eqref{eq:dynamic_pde}.
In this setting, the physical residual is given by
\begin{equation}
    \frac{\partial \Gt(a)}{\partial t}(x,t) - \mathcal{R}(\Gt(a)(x,t)),
\end{equation}
and the corresponding physics-informed loss is defined over the space--time domain,
\begin{equation}
    \begin{aligned}
        \Lphys(a, \Gt(a))
        :=& \int_0^T \int_D 
        \abs{
            \frac{\partial \Gt(a)}{\partial t}(x,t)
            - \mathcal{R}(\Gt(a)(x,t))
        }^2 \, \mathrm{d}x \, \mathrm{d}t + \alpha\int_0^T \int_{\partial D} 
        \abs{
            \Gt(a)(x,t)
            - g(x,t)
        }^2
        \, \mathrm{d}x \, \mathrm{d}t.\\
        & \beta \int_D \abs{
        \Gt(a)(x,0)-a(x)
        }^2\, \mathrm{d}x \, ,
    \end{aligned}  
\end{equation}
where $\alpha, \beta > 0$ are weighting hyper-parameters that balance the relative importance of satisfying the boundary and initial conditions, respectively, against the interior PDE residual.

The operator loss $\Jphys(\Gt)$ is again obtained by taking the expectation with respect to $\mu$.

\paragraph{Total training objective.}
The final loss functional used to train the Physics-Informed Neural Operator is given by a weighted combination of the data and physics losses,
\begin{equation}
    \Jtot(\Gt)
    := \Jdata(\Gt) + \lambda \, \Jphys(\Gt),
\end{equation}
where $\lambda > 0$ balances data fitting and physical consistency.

In practice, computing the time derivative of the neural operator output is non-trivial and requires additional modeling or numerical approximations, which we discuss in more detail below.


\subsection{Derivatives of Neural Operators}

For dynamical systems, the physics-informed loss requires the evaluation of the time derivative
\(
\partial_t \Gt(a)(x,t)
\).
This presents a fundamental difficulty for neural operators, as they learn mappings between function spaces rather than explicit space--time dependent solutions.

In contrast to classical PINNs, where the solution is represented as a neural network $u_\theta(x,t)$ and derivatives can be computed directly via automatic differentiation, neural operators produce solution trajectories as outputs. As a result, temporal derivatives are not readily available and must be approximated.

The Physics-Informed Neural Operator framework considers two main strategies to address this issue.

\paragraph{Discrete-time formulation.}
The first approach is based on a discrete-time or autoregressive formulation. The neural operator is trained to map the solution at one time step to the solution at the next time step. In this case, the time derivative is approximated using finite differences between consecutive outputs of the operator. This strategy avoids explicit differentiation with respect to time, but may lead to error accumulation when applied over long time intervals.

\paragraph{Continuous-time formulation.}
Alternatively, the neural operator can be extended to include time as an additional input variable, yielding a continuous-in-time representation of the solution. In this setting, the time derivative can be computed using automatic differentiation, allowing the PDE residual to be enforced directly. However, this approach increases computational cost and can be more difficult to scale to long-time dynamics.

Both formulations reflect a trade-off between numerical stability, computational efficiency, and accuracy. The appropriate choice depends on the structure of the problem and the temporal resolution required.

\subsection{From Theory to Computation}

We have introduced the operator-learning framework underlying neural operators and their physics-informed extensions. We formulated both stationary and dynamical PDEs from an operator perspective and described how data-driven and physics-based losses can be combined to train Physics-Informed Neural Operators.

While this formulation provides a flexible and expressive framework, its practical performance strongly depends on implementation choices, including the discretization of the domain, the treatment of derivatives, and the optimization strategy. In the following section, we turn to these computational aspects and discuss how PINNs and PINOs are implemented in practice, highlighting their similarities and differences.

\section{How can we solve PDEs using PINNs and PINOs?}

\subsection{PINNs}

\subsubsection{Why PINNs?}

PINNs have emerged in recent years as an effective framework for the solution of PDEs and related problems. By embedding the governing physical laws directly into the learning process, PINNs enforce physical consistency of the solution and reduce the reliance on large labeled datasets compared to conventional machine learning approaches. This physics-based regularization enables the treatment of nonlinear, high-dimensional PDEs, as well as coupled and multiphysics systems, within a unified framework. Moreover, PINNs circumvent the need for explicit mesh generation and yield continuous, differentiable approximations over the computational domain, which can lead to significant computational advantages over traditional numerical methods.

\subsubsection{Are PINNs better than traditional numerical methods?}

Despite the aforementioned advantages of physics-informed neural networks, several limitations remain, which have motivated ongoing discussion regarding their competitiveness with established numerical solvers for partial differential equations. Figure \ref{Traditional vs PINNs}, reproduced from Ren et al. \cite{app15148092}, provides a comparative assessment of traditional numerical methods, purely data-driven approaches, and PINNs across a set of PDE-oriented benchmarks, highlighting the respective strengths and weaknesses of each paradigm.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Imagenes/PINNs vs Traditional.png}
\caption{Paradigm Comparison: Traditional vs. Data-Driven vs. PINNs (evaluated on PDE-specific benchmarks). Reproduced from Ren et al. \cite{app15148092}}\label{Traditional vs PINNs}
\end{figure}

From this comparison, it can be observed that PINNs exhibit favorable performance with respect to computational cost, data efficiency, and generalization capabilities, while displaying computational complexity comparable to that of conventional numerical methods. However, PINNs typically lag behind classical solvers in terms of solution accuracy and interpretability, particularly in regimes where high-fidelity numerical discretizations are well established. These observations suggest that PINNs should not be regarded as a direct replacement for traditional numerical methods, but rather as a complementary modeling strategy, especially well suited for complex, high-dimensional, or data-scarce problems where conventional approaches may face significant challenges.

\subsubsection{PINNs workflow}

For solving a PDE using PINNs, the following workflow is typically followed:

\begin{itemize}
  \item \textbf{Problem Definition}: Clearly define the PDE, including the domain, boundary conditions, and any initial conditions.
  \item \textbf{Neural Network Architecture}: Design a neural network architecture suitable for the problem at hand. This typically involves selecting the number of layers, neurons per layer, and activation functions.
  \item \textbf{Loss Function Formulation}: Construct a loss function that incorporates the residuals of the PDE, boundary conditions, and initial conditions. This loss function quantifies how well the neural network satisfies the governing equations and constraints.
  \item \textbf{Model Training}: Train the neural network using optimization algorithms (e.g., Adam, L-BFGS) to minimize the loss function. This involves iteratively updating the network weights based on the computed gradients of the loss.
  \item \textbf{Model Evaluation and Validation}: After training, evaluate the performance of the PINN by comparing its predictions against known solutions or benchmark problems. This may involve computing error metrics and visualizing the results.
\end{itemize}

\subsection{Implementation Example}

In order to illustrate the PINNs workflow described above, we will now present a simple implementation example for solving a one-dimensional heat equation using PINNs.

\subsubsection{Problem Definition}

The first step is to define the PDE we want to solve. In this case, we will consider the one-dimensional heat equation given by:

\begin{equation}
\frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2}, \quad x \in [0, L], \quad t \in [0, T]
\end{equation}

with the following boundary and initial conditions:

\begin{equation}
u(0, t) = u(L, t) = 0, \quad u(x, 0) = \sin\left(\pi x\right)
\end{equation}

\subsubsection{Neural Network Architecture}

A fully connected feedforward neural network is employed to approximate the solution of the governing equation. The network takes the spatial coordinate \(x\) and time \(t\) as input features, and outputs a scalar field representing the temperature \(u(x, t)\). The architecture consists of three hidden layers with 50 neurons each, this choice represents a compromise between expressive capacity and computational cost, and is consistent with commonly adopted configurations in the literature for low-dimensional parabolic PDEs. The hyperbolic tangent (tanh) function is adopted as the activation function, owing to its smoothness and continuous differentiability, which are essential for the accurate evaluation of the spatial and temporal derivatives appearing in the PDE residual through automatic differentiation.

\subsubsection{Loss Function Formulation}

For the implementation of this simple example, we decided not to use any data for defining the loss function, which will be solely based on the physics of the problem. In this way, the loss function \( \mathcal{L} \) is constructed as follows:

\begin{equation}
\mathcal{L} = \mathcal{L}_{PDE} + \mathcal{L}_{BC} + \mathcal{L}_{IC}
\end{equation}
where
\begin{equation}
\mathcal{L}_{PDE} = \frac{1}{N_f} \sum_{i=1}^{N_f} \left| \frac{\partial u}{\partial t}(x_i, t_i;\theta) - \alpha \frac{\partial^2 u}{\partial x^2}(x_i, t_i;\theta) \right|^2
\end{equation}
\begin{equation}
\mathcal{L}_{BC} = \frac{1}{N_b} \sum_{i=1}^{N_b} \left( |u(0, t_i;\theta)|^2 + |u(L, t_i;\theta)|^2 \right)
\end{equation}
\begin{equation}
\mathcal{L}_{IC} = \frac{1}{N_0} \sum_{i=1}^{N_0} \left| u(x_i, 0;\theta) - \sin(\pi x_i) \right|^2
\end{equation}

Here, \(N_f\), \(N_b\), and \(N_0\) denote the number of collocation points for the PDE residual, boundary conditions, and initial conditions, respectively. The terms \( \mathcal{L}_{PDE} \), \( \mathcal{L}_{BC} \), and \( \mathcal{L}_{IC} \) represent the contributions to the total loss from the PDE residual, boundary conditions, and initial conditions, respectively.

\subsubsection{Model Training}

The neural network is trained using the Adam optimizer, which is well-suited for handling the non-convex optimization landscape typical of deep learning models. The training process involves iteratively updating the network parameters \( \theta \) to minimize the total loss \( \mathcal{L} \). An initial learning rate of \( 0.001 \) is employed, and the training is conducted over \( 6,000 \) epochs, with a decay rate of 0.9 every 1000 epochs. During each epoch, the gradients of the loss function with respect to the network parameters are computed using automatic differentiation, enabling efficient backpropagation through the network. Since this model is prettty simple we do not use any stopping criteria other than the maximum number of epochs.

Figure \ref{PINN Architecture}, reproduced from Luo et al. \cite{Luo2025}, illustrates the general framework of physics-informed neural networks, summarizing in a unified schematic the main components of the methodology described above, including the neural network approximation, the incorporation of governing equations and constraints, and the optimization process.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Imagenes/PINN Architecture.png}
\caption{The framework of PINNs. Reproduced from Luo et al. \cite{Luo2025}}\label{PINN Architecture}
\end{figure}

\subsubsection{Model Evaluation and Validation}

In order to check that the training has been completed successfully, we can visualize the loss evolution during the training process. Figure \ref{Loss Evolution} shows how the loss decreases as the number of epochs increases, indicating that the neural network is learning to satisfy the governing equations and constraints.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{Imagenes/LossFunctionvsEpoch.png}
  \caption{Loss evolution during training}\label{Loss Evolution}
\end{figure}

This decreasing trend in the loss function suggests that the PINN is effectively capturing the underlying physics of the heat equation. And the flat region at the end indicates that the model has converged to a solution.

After this, we can obtain the solution of the PDE by evaluating the trained neural network at desired spatial and temporal locations. Figure \ref{PINN Analytical} shows the solutions obtained by the PINN, on the right side, and analitically on the left.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Imagenes/PINN solution.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Imagenes/Analytical Solution.png}
    \end{minipage}
    \caption{Solutions obtained by the PINN and analitically}\label{PINN Analytical}
\end{figure}

For comparing both solutions, we can substract them and obtain the error, which is shown in Figure \ref{Error PINN}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{Imagenes/Error PINNvsAnalytical.png}
  \caption{Error between PINN and analytical solution}\label{Error PINN}
\end{figure}

Analizing the error, we can see that it is pretty low, being the maximum error around \(4 \cdot 10^{-2}\). This shows that PINNs can be a good alternative for solving PDEs.

We can also visualize the comparison between the PINN and analytical solutions at specific time instances. Figure \ref{Comparison at t} illustrates the temperature distribution along the spatial domain at different times, demonstrating the close agreement between the two solutions.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{Imagenes/Temporal slices.png}
  \caption{Comparison of PINN and analytical solutions at different times}\label{Comparison at t}
\end{figure}

For a more precise evaluation of the PINNs performance, we compute the root mean squared error (RMSE) between the PINN-predicted solution and the analytical solution over a dense grid of spatial and temporal points. The RMSE is defined as:

\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \left( u_{\text{PINN}}(x_i, t_i) - u_{\text{analytical}}(x_i, t_i) \right)^2}
\end{equation}

where \(N\) is the total number of evaluation points, \(u_{\text{PINN}}\) is the solution predicted by the PINN, and \(u_{\text{analytical}}\) is the exact analytical solution. For our implementation, we obtained an RMSE value of approximately \(8.71 \times 10^{-4}\), indicating a high level of accuracy in the PINN's approximation of the heat equation solution.

By these analyses, we can conclude that PINNs provide a viable and accurate approach for solving PDEs, effectively capturing the underlying physics of the problem while maintaining computational efficiency, although its precission may not yet rival that of traditional numerical methods in all scenarios. The distance between this two approaches may be reduced by not only using physical constraints but also data, leading to the so-called data-enhanced PINNs.

\subsection{Transition to PINOs}

\subsubsection{Why PINOs?}

While PINNs have demonstrated to be effective for solving a variety of PDEs, they are not without limitations. Some of the main banefit provided by PINOs are: The parameter space generalization capabilities of PINOs allow them to efficiently handle a wide range of parameterized PDEs without the need for retraining, making them particularly suitable for scenarios where multiple simulations are required across different parameter settings; they can also handle complex domains, adapting to irregular, moving or parametrized geometries more naturally than PINNs; furthermore, PINOs offer real-time inference capabilities, enabling rapid predictions once trained, which is advantageous for applications requiring quick evaluations, such as control systems or real-time monitoring; lastly, this kind of models can cope with multi-query scenarios, efficiently managing multiple input conditions or source terms within a single trained model.

\subsubsection{Differences between PINNs and PINOs}

The main difference between PINNs and PINOs lies in their approach to solving PDEs. PINNs use a single neural network to approximate the solution of a specific PDE, while PINOs employ operator learning techniques to learn the mapping from input functions (e.g., initial conditions, boundary conditions, source terms) to the solution of the PDE. This allows PINOs to generalize across different instances of the same PDE with varying inputs, making them more versatile for problems where multiple simulations are required.

This difference in approaching the problems doesn't mean that PINOs are better than PINNs, as both methods have their own strengths and weaknesses. The choice between using a PINN or a PINO depends on the specific problem at hand, the availability of data, and the desired generalization capabilities.

In Table \ref{tab:methods_comparison}, reproduced from Zhang et al. \cite{zhang2025physicsinformedneuralnetworksneural}, we summarize some of the main methods used in different domains along with their reported speedup compared to traditional numerical methods.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l c}
    \hline
    \textbf{Domain} & \textbf{Methods} & \textbf{Reported Speedup} \\ 
    \hline
    Fluid Dynamics & FNO, DeepONet, PINO & $10^{3}-10^{5}$ $\times$ \\ 
    
    Solid Mechanics & PINN, DeepONet, GNO & $10^{2}-10^{3}$ $\times$ \\ 
    
    Heat Transfer & PI-DeepONet, PINN & $10^{3}-10^{4}$ $\times$ \\ 
    
    Electromagnetics & PINN, MaxwellNet & $10^{2}-10^{3}$ $\times$ \\ 
    \hline
    \end{tabular}
    \caption{Cross-domain comparison of neural methods for parametric PDEs. Reproduced from Zhang et al. \cite{zhang2025physicsinformedneuralnetworksneural}}\label{tab:methods_comparison}
\end{table}

\subsubsection{PINOs workflow}

The workflow for implementing PINOs shares similarities with that of PINNs, but with key differences in the model architectures. The following steps outline the typical workflow for implementing a PINO:

\begin{itemize}
  \item \textbf{Problem Definition}: Define the PDE and the range of input functions (e.g., initial conditions, boundary conditions) that the PINO will learn to map to solutions.
  \item \textbf{Operator Network Architecture}: Design a neural network architecture capable of learning the operator mapping from input functions to PDE solutions. This may involve using architectures like DeepONets or Fourier Neural Operators (FNOs).
  \item \textbf{Loss Function Formulation}: Construct a loss function that measures the discrepancy between the predicted solutions and the true solutions for a set of training input functions. This typically involves sampling a diverse set of input functions and their corresponding solutions.
  \item \textbf{Model Training}: Train the operator network using optimization algorithms to minimize the loss function. This involves iteratively updating the network weights based on the computed gradients of the loss.
  \item \textbf{Model Evaluation and Validation}: Evaluate the performance of the trained PINO by testing its ability to generalize to unseen input functions and comparing its predictions against known solutions or benchmark problems.
\end{itemize}

\subsection{Conclusions}

In this work, we have explored the methodologies of Physics-Informed Neural Networks (PINNs) and Physics-Informed Neural Operators (PINOs) for solving partial differential equations (PDEs). We have discussed the advantages and limitations of each approach, highlighting their respective strengths in handling complex physical systems. Through a detailed implementation example of a PINN applied to the one-dimensional heat equation, we demonstrated the practical workflow involved in defining the problem, designing the neural network architecture, formulating the loss function, training the model, and evaluating its performance. The results showcased the capability of PINNs to accurately approximate PDE solutions while adhering to physical laws. Furthermore, we outlined the transition to PINOs, emphasizing their operator learning framework that enables generalization across varying input conditions. Overall, both PINNs and PINOs represent promising avenues for integrating machine learning with traditional numerical methods in scientific computing, offering new tools for tackling challenging PDE problems.

%% For citations use: 
%%       \cite{<label>} ==> [1]

%%
%% Example citation, See \cite{lamport94}.

%% If you have bib database file and want bibtex to generate the
%% bibitems, please use
%%
  \bibliographystyle{elsarticle-num} 
  \bibliography{referencias}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

%%\begin{thebibliography}{00}

%% For numbered reference style
%% \bibitem{label}
%% Text of bibliographic item

%%\bibitem{lamport94}
%%  Leslie Lamport,
%%  \textit{\LaTeX: a document preparation system},
%%  Addison Wesley, Massachusetts,
%%  2nd edition,
%%  1994.

%%\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
